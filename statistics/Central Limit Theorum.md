---
created: 2021-01-09T14:04:57+10:00
modified: 2021-01-09T14:05:25+10:00
---

# Central Limit Theorum

*From Naked Statistics by Charles Wheelan*

We can test a hundred chicken breasts for salmonella at a poultry processing plant and conclude from that sample alone that the entire plant is safe or unsafe. Where does this extraordinary power to generalize come from?
Much of it comes from the central limit theorem...
The central limit theorem is the “power source” for many of the statistical activities that involve using a sample to make inferences about a large population
These kinds of inference are just a combination of two tools that we’ve already explored: probability and proper sampling.

And if you understand the central limit theorem, most forms of statistical inference will seem relatively intuitive.

The core principle underlying the central limit theorem is that a large, properly drawn sample will resemble the population from which it is drawn

The central limit theorem enables us to make the following inference:
1. If we have detailed information about some population, then we can make powerful inferences about any properly drawn sample from that population
2. If we have detailed information about a properly drawn sample (mean and standard deviation), we can make strikingly accurate inferences about the population from which that sample was drawn. The central limit theorem tells us that a large sample will not typically deviate sharply from its underlying population — which means that sample results are a good proxy for the results of the population overall
3. If we have data describing a particular sample, and data on a particular population, we can infer whether or not that sample is consistent with a sample that is likely to be drawn from that population. The central limit theorem enables us to calculate the probability that a particular sample was drawn from a given population. If that probability is low, then we can conclude with a high degree of confidence that the sample was not drawn from the population in question.
4. Last, if we know the underlying characteristics of two samples, we can infer whether or not both samples were likely drawn from the same population.

The central limit theorem tells us that the sample means will be distributed roughly as a normal distribution around the population mean. The normal distribution has as 68 percent of the observations lie within one standard deviation of the mean, 95 percent lie within two standard deviations, and so on.
All of this will be true no matter what the distribution of the underlying population looks like. 

> The population from which the samples are being drawn does not have to have a normal distribution in order for the sample means to be distributed normally.

The central limit theorem enables us to go one step further by describing the expected distribution of those different sample means as they cluster around the population mean. Specifically, the sample means will form a normal distribution around the population mean, which in this case is $70,900. Remember, the shape of the underlying population doesn’t matter.

The larger the sample size and the more samples taken, the more closely the distribution of sample means will approximate the normal curve.

A larger sample is less likely to be affected by random variation.